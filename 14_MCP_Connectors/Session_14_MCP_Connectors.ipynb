{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 14: MCP-Powered X Post Summarizer\n",
    "\n",
    "## Building a LangGraph Agent with GitHub MCP Tools, X API Tools, and Memory\n",
    "\n",
    "## Learning Objectives:\n",
    "\n",
    "- **Ingest MCP servers as LangGraph tools** using `langchain-mcp-adapters` to connect to the GitHub MCP Server and use its tools programmatically\n",
    "- **Wrap the X (Twitter) API as a LangChain tool** using the `@tool` decorator so a LangGraph agent can search and retrieve public posts\n",
    "- **Build a LangGraph agent with memory** that combines MCP-sourced tools and custom tools, using `MemorySaver` for short-term conversational memory\n",
    "- **Orchestrate a full workflow through the agent** â€” search X posts, generate summaries, create a GitHub repo, commit files, branch, and open a PR â€” all via natural language\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, you will build a **LangGraph ReAct agent** that has access to two categories of tools:\n",
    "\n",
    "1. **GitHub MCP tools** â€” loaded from the official GitHub MCP Server via `langchain-mcp-adapters`. These replace manual `git` commands with tool calls the agent can invoke (create repos, commit files, create branches, open PRs).\n",
    "2. **X API tools** â€” custom Python functions wrapped with the `@tool` decorator that call the X API v2 directly to search and retrieve posts.\n",
    "\n",
    "The agent uses **`MemorySaver`** for short-term memory so it can maintain context across multi-step workflows within a conversation thread.\n",
    "\n",
    "There will be one breakout room with two phases:\n",
    "\n",
    "- ðŸ¤ Phase 1: Setup, Tools & Agent Construction\n",
    "  - Task 1: Dependencies & Environment\n",
    "  - Task 2: X API as LangChain Tools\n",
    "  - Task 3: Connect to GitHub MCP Server & Load Tools\n",
    "  - Task 4: Build the LangGraph Agent with Memory\n",
    "  - Task 5: Test the Agent â€” Search & Summarize X Posts\n",
    "  - Activity #1: Extend the Agent with a Custom X API Tool\n",
    "- ðŸ¤ Phase 2: MCP Workflow Through the Agent\n",
    "  - Task 6: Create a GitHub Repository\n",
    "  - Task 7: Commit the Summary\n",
    "  - Task 8: Create a Feature Branch & Add Metadata\n",
    "  - Task 9: Open a Pull Request\n",
    "  - Task 10: Commit the X API Script\n",
    "  - Task 11: Update the README\n",
    "  - Activity #1: Multi-Account Comparison Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ¤ Breakout Room \n",
    "## Setup, Tools & Agent Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Dependencies & Environment\n",
    "\n",
    "We need:\n",
    "- `langchain-mcp-adapters` to connect to MCP servers and convert their tools into LangChain tools\n",
    "- `langgraph` for our agent graph with memory\n",
    "- `langchain-openai` for our LLM\n",
    "- `requests` for the X API calls\n",
    "- `nest-asyncio` for async MCP operations inside Jupyter\n",
    "\n",
    "> NOTE: Create a `.env` file in this directory with `X_BEARER_TOKEN`, `OPENAI_API_KEY`, and `GITHUB_PAT` before running.\n",
    "> \n",
    "> Setup references:\n",
    "> - GitHub fine-grained PAT guide: [Creating a personal access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-fine-grained-personal-access-token)\n",
    "> - X API Bearer Token setup: [X Developer Portal](https://developer.x.com/en/portal/dashboard) and [X API access tiers](https://developer.x.com/en/products/twitter-api)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohere API key set\n",
      "Anthropic API key set\n",
      "OpenAI API key set\n",
      "LangSmith tracing enabled. Project: AIE9 - Deep Agents - e4efe392\n"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def get_api_key(env_var: str, prompt: str) -> str:\n",
    "    \"\"\"Get API key from environment or prompt user.\"\"\"\n",
    "    value = os.environ.get(env_var, \"\")\n",
    "    if not value:\n",
    "        value = getpass.getpass(prompt)\n",
    "        if value:\n",
    "            os.environ[env_var] = value\n",
    "    return value\n",
    "\n",
    "cohere_key = get_api_key(\"COHERE_API_KEY\", \"Cohere API Key: \")\n",
    "if cohere_key:\n",
    "    print(\"cohere API key set\")\n",
    "else:\n",
    "    print(\"Warning: No Cohere API key configured\")\n",
    "\n",
    "# Set Anthropic API Key (default for Deep Agents)\n",
    "anthropic_key = get_api_key(\"ANTHROPIC_API_KEY\", \"Anthropic API Key: \")\n",
    "if anthropic_key:\n",
    "    print(\"Anthropic API key set\")\n",
    "else:\n",
    "    print(\"Warning: No Anthropic API key configured\")\n",
    "\n",
    "# Optional: OpenAI for alternative models and subagents\n",
    "openai_key = get_api_key(\"OPENAI_API_KEY\", \"OpenAI API Key (press Enter to skip): \")\n",
    "if openai_key:\n",
    "    print(\"OpenAI API key set\")\n",
    "else:\n",
    "    print(\"OpenAI API key not configured (optional)\")\n",
    "\n",
    "# Optional: LangSmith for tracing\n",
    "langsmith_key = get_api_key(\"LANGCHAIN_API_KEY\", \"LangSmith API Key (press Enter to skip): \")\n",
    "\n",
    "if langsmith_key:\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE9 - Deep Agents - {uuid4().hex[0:8]}\"\n",
    "    print(f\"LangSmith tracing enabled. Project: {os.environ['LANGCHAIN_PROJECT']}\")\n",
    "else:\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "    print(\"LangSmith tracing disabled\")\n",
    "\n",
    "if not os.environ.get(\"METAL_API_KEY\"):\n",
    "    os.environ[\"METAL_API_KEY\"] = getpass(\"Please enter your metals.dev API key!\")\n",
    "\n",
    "if not os.environ.get(\"X_BEARER_TOKEN\"):\n",
    "    os.environ[\"X_BEARER_TOKEN\"] = getpass.getpass(\"Enter your X Bearer Token:\")\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")\n",
    "\n",
    "if not os.environ.get(\"GITHUB_PAT\"):\n",
    "    os.environ[\"GITHUB_PAT\"] = getpass.getpass(\"Enter your GitHub PAT:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Required for async operations in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Your Credentials\n",
    "\n",
    "**GitHub PAT (fine-grained):**\n",
    "1. Open [GitHub Personal Access Tokens (fine-grained)](https://github.com/settings/personal-access-tokens/new).\n",
    "2. Follow [GitHub's PAT setup guide](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-fine-grained-personal-access-token).\n",
    "3. Set repository permissions to at least:\n",
    "   - `Contents`: Read and write\n",
    "   - `Pull requests`: Read and write\n",
    "   - `Metadata`: Read-only\n",
    "\n",
    "**X Bearer Token:**\n",
    "1. Open the [X Developer Portal](https://developer.x.com/en/portal/dashboard).\n",
    "2. Create/select a Project + App, then go to **Keys and Tokens** to generate a Bearer Token.\n",
    "3. Confirm your plan supports the recent search endpoint (`GET /2/tweets/search/recent`) from the [X API product page](https://developer.x.com/en/products/twitter-api).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP agent ready!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# Test the connection\n",
    "response = llm.invoke(\"Say 'MCP agent ready!' in exactly those words.\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: X API as LangChain Tools\n",
    "\n",
    "Instead of relying on a community-built MCP server for X, we'll call the **X API v2** directly and wrap our functions with the `@tool` decorator. This makes them available to our LangGraph agent as callable tools â€” just like the MCP tools will be.\n",
    "\n",
    "This is a key architectural decision: **not everything needs to be an MCP server**. Wrapping a simple API call as a `@tool` is often simpler and more transparent.\n",
    "\n",
    "**ðŸ“š Documentation:**\n",
    "- [LangChain Tools Conceptual Guide](https://python.langchain.com/docs/concepts/tools/)\n",
    "- [X API v2 Documentation](https://developer.x.com/en/docs/x-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2 X API tools: ['search_recent_posts', 'get_user_posts']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "BEARER_TOKEN = os.environ.get(\"X_BEARER_TOKEN\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_recent_posts(query: str, max_results: int = 20) -> str:\n",
    "    \"\"\"Search recent X/Twitter posts using the v2 API.\n",
    "    Returns posts from the last 7 days matching the query.\n",
    "    Use this for keyword searches, hashtag searches, or general topic searches.\n",
    "\n",
    "    Args:\n",
    "        query: The search query (e.g., 'AI safety', '#machinelearning', 'from:AndrewYNg')\n",
    "        max_results: Number of results to return (10-100, default 20)\n",
    "    \"\"\"\n",
    "    url = \"https://api.x.com/2/tweets/search/recent\"\n",
    "    headers = {\"Authorization\": f\"Bearer {BEARER_TOKEN}\"}\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"max_results\": min(max(max_results, 10), 100),\n",
    "        \"tweet.fields\": \"created_at,public_metrics,author_id,text\",\n",
    "        \"expansions\": \"author_id\",\n",
    "        \"user.fields\": \"name,username\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    tweets = data.get(\"data\", [])\n",
    "    if not tweets:\n",
    "        return \"No posts found for this query.\"\n",
    "\n",
    "    result_lines = [f\"Found {len(tweets)} posts:\\n\"]\n",
    "    for t in tweets:\n",
    "        metrics = t.get(\"public_metrics\", {})\n",
    "        result_lines.append(\n",
    "            f\"[{t.get('created_at', 'unknown')[:10]}] \"\n",
    "            f\"{t['text'][:200]}\\n\"\n",
    "            f\"  Likes: {metrics.get('like_count', 0)} | \"\n",
    "            f\"Retweets: {metrics.get('retweet_count', 0)}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(result_lines)\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_user_posts(username: str, max_results: int = 20) -> str:\n",
    "    \"\"\"Get recent original posts (no retweets) from a specific X/Twitter user.\n",
    "    Use this when you want to see what a specific account has been posting.\n",
    "\n",
    "    Args:\n",
    "        username: The X/Twitter handle without the @ sign (e.g., 'AndrewYNg')\n",
    "        max_results: Number of results to return (10-100, default 20)\n",
    "    \"\"\"\n",
    "    query = f\"from:{username} -is:retweet\"\n",
    "    return search_recent_posts.invoke({\"query\": query, \"max_results\": max_results})\n",
    "\n",
    "\n",
    "x_api_tools = [search_recent_posts, get_user_posts]\n",
    "print(f\"Created {len(x_api_tools)} X API tools: {[t.name for t in x_api_tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify our X API tools work before wiring them into the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://api.x.com/2/tweets/search/recent?query=from%3Allm_wizard+-is%3Aretweet&max_results=10&tweet.fields=created_at%2Cpublic_metrics%2Cauthor_id%2Ctext&expansions=author_id&user.fields=name%2Cusername",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Quick test â€” fetch recent posts from a public account\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result = \u001b[43mget_user_posts\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43musername\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllm_wizard\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(result[:\u001b[32m500\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/old/personal/code/ai-makerspace-code/AIE9/14_MCP_Connectors/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:642\u001b[39m, in \u001b[36mBaseTool.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    636\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    639\u001b[39m     **kwargs: Any,\n\u001b[32m    640\u001b[39m ) -> Any:\n\u001b[32m    641\u001b[39m     tool_input, kwargs = _prep_run_args(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/old/personal/code/ai-makerspace-code/AIE9/14_MCP_Connectors/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:1001\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m   1000\u001b[39m     run_manager.on_tool_error(error_to_raise, tool_call_id=tool_call_id)\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m   1002\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m   1003\u001b[39m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/old/personal/code/ai-makerspace-code/AIE9/14_MCP_Connectors/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:967\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m._run):\n\u001b[32m    966\u001b[39m         tool_kwargs |= {config_param: config}\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     response = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.response_format == \u001b[33m\"\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    969\u001b[39m     msg = (\n\u001b[32m    970\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSince response_format=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    971\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33ma two-tuple of the message content and raw tool output is \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    972\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected. Instead, generated response is of type: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    973\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(response)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    974\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/old/personal/code/ai-makerspace-code/AIE9/14_MCP_Connectors/.venv/lib/python3.11/site-packages/langchain_core/tools/structured.py:97\u001b[39m, in \u001b[36mStructuredTool._run\u001b[39m\u001b[34m(self, config, run_manager, *args, **kwargs)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m.func):\n\u001b[32m     96\u001b[39m         kwargs[config_param] = config\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mStructuredTool does not support sync invocation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mget_user_posts\u001b[39m\u001b[34m(username, max_results)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get recent original posts (no retweets) from a specific X/Twitter user.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03mUse this when you want to see what a specific account has been posting.\u001b[39;00m\n\u001b[32m     52\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m \u001b[33;03m    max_results: Number of results to return (10-100, default 20)\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     57\u001b[39m query = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfrom:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -is:retweet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msearch_recent_posts\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/old/personal/code/ai-makerspace-code/AIE9/14_MCP_Connectors/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:642\u001b[39m, in \u001b[36mBaseTool.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    636\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    639\u001b[39m     **kwargs: Any,\n\u001b[32m    640\u001b[39m ) -> Any:\n\u001b[32m    641\u001b[39m     tool_input, kwargs = _prep_run_args(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/old/personal/code/ai-makerspace-code/AIE9/14_MCP_Connectors/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:1001\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m   1000\u001b[39m     run_manager.on_tool_error(error_to_raise, tool_call_id=tool_call_id)\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m   1002\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m   1003\u001b[39m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/old/personal/code/ai-makerspace-code/AIE9/14_MCP_Connectors/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:967\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m._run):\n\u001b[32m    966\u001b[39m         tool_kwargs |= {config_param: config}\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     response = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.response_format == \u001b[33m\"\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    969\u001b[39m     msg = (\n\u001b[32m    970\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSince response_format=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    971\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33ma two-tuple of the message content and raw tool output is \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    972\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected. Instead, generated response is of type: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    973\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(response)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    974\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/old/personal/code/ai-makerspace-code/AIE9/14_MCP_Connectors/.venv/lib/python3.11/site-packages/langchain_core/tools/structured.py:97\u001b[39m, in \u001b[36mStructuredTool._run\u001b[39m\u001b[34m(self, config, run_manager, *args, **kwargs)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m.func):\n\u001b[32m     96\u001b[39m         kwargs[config_param] = config\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mStructuredTool does not support sync invocation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36msearch_recent_posts\u001b[39m\u001b[34m(query, max_results)\u001b[39m\n\u001b[32m     20\u001b[39m params = {\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: query,\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_results\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(max_results, \u001b[32m10\u001b[39m), \u001b[32m100\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muser.fields\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mname,username\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m }\n\u001b[32m     28\u001b[39m response = requests.get(url, headers=headers, params=params)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m data = response.json()\n\u001b[32m     32\u001b[39m tweets = data.get(\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m, [])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/old/personal/code/ai-makerspace-code/AIE9/14_MCP_Connectors/.venv/lib/python3.11/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://api.x.com/2/tweets/search/recent?query=from%3Allm_wizard+-is%3Aretweet&max_results=10&tweet.fields=created_at%2Cpublic_metrics%2Cauthor_id%2Ctext&expansions=author_id&user.fields=name%2Cusername"
     ]
    }
   ],
   "source": [
    "# Quick test â€” fetch recent posts from a public account\n",
    "result = get_user_posts.invoke({\"username\": \"llm_wizard\", \"max_results\": 10})\n",
    "print(result[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Connect to GitHub MCP Server & Load Tools\n",
    "\n",
    "Now we'll connect to the **GitHub MCP Server** â€” an official, GitHub-maintained MCP server that gives agents the ability to manage repositories, issues, pull requests, and more.\n",
    "\n",
    "We use `langchain-mcp-adapters` to:\n",
    "1. Connect to the remote GitHub MCP server over HTTP\n",
    "2. Automatically convert all MCP tools into LangChain-compatible tools\n",
    "\n",
    "This is the key MCP integration point â€” instead of writing custom GitHub API wrappers, we get a full set of tools for free just by connecting to the MCP server.\n",
    "\n",
    "**ðŸ“š Documentation:**\n",
    "- [langchain-mcp-adapters](https://github.com/langchain-ai/langchain-mcp-adapters)\n",
    "- [GitHub MCP Server](https://github.com/github/github-mcp-server)\n",
    "- [Model Context Protocol Specification](https://modelcontextprotocol.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 40 GitHub MCP tools:\n",
      "\n",
      "  - add_comment_to_pending_review: Add review comment to the requester's latest pending pull request review. A pend...\n",
      "  - add_issue_comment: Add a comment to a specific issue in a GitHub repository. Use this tool to add c...\n",
      "  - add_reply_to_pull_request_comment: Add a reply to an existing pull request comment. This creates a new comment that...\n",
      "  - create_branch: Create a new branch in a GitHub repository...\n",
      "  - create_or_update_file: Create or update a single file in a GitHub repository. \n",
      "If updating, you should ...\n",
      "  - create_pull_request: Create a new pull request in a GitHub repository....\n",
      "  - create_repository: Create a new GitHub repository in your account or specified organization...\n",
      "  - delete_file: Delete a file from a GitHub repository...\n",
      "  - fork_repository: Fork a GitHub repository to your account or specified organization...\n",
      "  - get_commit: Get details for a commit from a GitHub repository...\n",
      "  - get_file_contents: Get the contents of a file or directory from a GitHub repository...\n",
      "  - get_label: Get a specific label from a repository....\n",
      "  - get_latest_release: Get the latest release in a GitHub repository...\n",
      "  - get_me: Get details of the authenticated GitHub user. Use this when a request is about t...\n",
      "  - get_release_by_tag: Get a specific release by its tag name in a GitHub repository...\n",
      "  - get_tag: Get details about a specific git tag in a GitHub repository...\n",
      "  - get_team_members: Get member usernames of a specific team in an organization. Limited to organizat...\n",
      "  - get_teams: Get details of the teams the user is a member of. Limited to organizations acces...\n",
      "  - issue_read: Get information about a specific issue in a GitHub repository....\n",
      "  - issue_write: Create a new or update an existing issue in a GitHub repository....\n",
      "  - list_branches: List branches in a GitHub repository...\n",
      "  - list_commits: Get list of commits of a branch in a GitHub repository. Returns at least 30 resu...\n",
      "  - list_issue_types: List supported issue types for repository owner (organization)....\n",
      "  - list_issues: List issues in a GitHub repository. For pagination, use the 'endCursor' from the...\n",
      "  - list_pull_requests: List pull requests in a GitHub repository. If the user specifies an author, then...\n",
      "  - list_releases: List releases in a GitHub repository...\n",
      "  - list_tags: List git tags in a GitHub repository...\n",
      "  - merge_pull_request: Merge a pull request in a GitHub repository....\n",
      "  - pull_request_read: Get information on a specific pull request in GitHub repository....\n",
      "  - pull_request_review_write: Create and/or submit, delete review of a pull request.\n",
      "\n",
      "Available methods:\n",
      "- cre...\n",
      "  - push_files: Push multiple files to a GitHub repository in a single commit...\n",
      "  - request_copilot_review: Request a GitHub Copilot code review for a pull request. Use this for automated ...\n",
      "  - search_code: Fast and precise code search across ALL GitHub repositories using GitHub's nativ...\n",
      "  - search_issues: Search for issues in GitHub repositories using issues search syntax already scop...\n",
      "  - search_pull_requests: Search for pull requests in GitHub repositories using issues search syntax alrea...\n",
      "  - search_repositories: Find GitHub repositories by name, description, readme, topics, or other metadata...\n",
      "  - search_users: Find GitHub users by username, real name, or other profile information. Useful f...\n",
      "  - sub_issue_write: Add a sub-issue to a parent issue in a GitHub repository....\n",
      "  - update_pull_request: Update an existing pull request in a GitHub repository....\n",
      "  - update_pull_request_branch: Update the branch of a pull request with the latest changes from the base branch...\n"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Connect to the GitHub MCP server using Streamable HTTP transport\n",
    "# The server exposes GitHub operations as MCP tools that our agent can call\n",
    "mcp_client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"github\": {\n",
    "            \"transport\": \"http\",\n",
    "            \"url\": \"https://api.githubcopilot.com/mcp/\",\n",
    "            \"headers\": {\n",
    "                \"Authorization\": f\"Bearer {os.environ['GITHUB_PAT']}\",\n",
    "            },\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Load all tools from the MCP server\n",
    "github_mcp_tools = await mcp_client.get_tools()\n",
    "\n",
    "print(f\"Loaded {len(github_mcp_tools)} GitHub MCP tools:\\n\")\n",
    "for t in github_mcp_tools:\n",
    "    print(f\"  - {t.name}: {t.description[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key GitHub MCP Tools\n",
    "\n",
    "The MCP server exposes many tools, but the key ones we'll use are:\n",
    "\n",
    "| MCP Tool | Replaces (Git CLI) | What It Does |\n",
    "|---|---|---|\n",
    "| `create_repository` | `git init` + GitHub UI | Creates a new repo on your account |\n",
    "| `create_or_update_file` | `git add` + `git commit` + `git push` | Commits a file directly to a branch |\n",
    "| `create_branch` | `git checkout -b` | Creates a new branch |\n",
    "| `create_pull_request` | `gh pr create` | Opens a PR from one branch to another |\n",
    "| `search_repositories` | `gh repo list` | Searches across your repos |\n",
    "| `get_file_contents` | `git show` / `cat` | Reads a file from a repo |\n",
    "| `list_commits` | `git log` | Shows commit history |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Build the LangGraph Agent with Memory\n",
    "\n",
    "Now we combine **both tool sets** into a single LangGraph agent:\n",
    "- **X API tools** â€” custom `@tool` functions for searching posts\n",
    "- **GitHub MCP tools** â€” loaded from the MCP server via `langchain-mcp-adapters`\n",
    "\n",
    "We add **`MemorySaver`** for short-term memory so the agent remembers context across the multi-step workflow (e.g., it fetches posts in one turn, summarizes them in the next, and commits the summary in a third).\n",
    "\n",
    "The architecture follows the standard LangGraph ReAct pattern from Sessions 4-6:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  START   â”‚â”€â”€â”€â”€â–¶â”‚   Agent   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  (LLM +   â”‚               â”‚\n",
    "                â”‚   tools)  â”‚               â”‚\n",
    "                â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â”‚\n",
    "                      â”‚                     â”‚\n",
    "               has tool calls?              â”‚\n",
    "                /           \\               â”‚\n",
    "              yes            no             â”‚\n",
    "              /               \\             â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n",
    "    â”‚  Tool Node  â”‚     â”‚   END   â”‚        â”‚\n",
    "    â”‚ (X API +    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\n",
    "    â”‚  GitHub MCP)â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**ðŸ“š Documentation:**\n",
    "- [LangGraph ReAct Agent](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\n",
    "- [MemorySaver (Checkpointing)](https://langchain-ai.github.io/langgraph/concepts/persistence/)\n",
    "- [ToolNode Prebuilt](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tools available to agent: 42\n",
      "  X API tools: ['search_recent_posts', 'get_user_posts']\n",
      "  GitHub MCP tools: ['add_comment_to_pending_review', 'add_issue_comment', 'add_reply_to_pull_request_comment', 'create_branch', 'create_or_update_file', 'create_pull_request', 'create_repository', 'delete_file', 'fork_repository', 'get_commit', 'get_file_contents', 'get_label', 'get_latest_release', 'get_me', 'get_release_by_tag', 'get_tag', 'get_team_members', 'get_teams', 'issue_read', 'issue_write', 'list_branches', 'list_commits', 'list_issue_types', 'list_issues', 'list_pull_requests', 'list_releases', 'list_tags', 'merge_pull_request', 'pull_request_read', 'pull_request_review_write', 'push_files', 'request_copilot_review', 'search_code', 'search_issues', 'search_pull_requests', 'search_repositories', 'search_users', 'sub_issue_write', 'update_pull_request', 'update_pull_request_branch']\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Combine all tools: X API tools + GitHub MCP tools\n",
    "all_tools = x_api_tools + github_mcp_tools\n",
    "print(f\"Total tools available to agent: {len(all_tools)}\")\n",
    "print(f\"  X API tools: {[t.name for t in x_api_tools]}\")\n",
    "print(f\"  GitHub MCP tools: {[t.name for t in github_mcp_tools]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent compiled with memory and tools!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the Agent State\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# Step 2: Define the system prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are an AI assistant that can search X/Twitter posts and manage GitHub repositories.\n",
    "\n",
    "You have two categories of tools:\n",
    "1. X API tools: search_recent_posts, get_user_posts â€” for searching and retrieving X/Twitter posts\n",
    "2. GitHub MCP tools: for creating repos, committing files, creating branches, opening PRs, etc.\n",
    "\n",
    "When asked to summarize posts, retrieve them first using the X API tools, then provide a structured\n",
    "markdown summary with: Overview, Key Themes, Notable Posts, and Summary Statistics.\n",
    "\n",
    "When asked to perform GitHub operations, use the appropriate GitHub MCP tool.\n",
    "Always use the available tools when appropriate. Be concise in your responses.\"\"\"\n",
    "\n",
    "\n",
    "# Step 3: Bind tools to the LLM\n",
    "llm_with_tools = llm.bind_tools(all_tools)\n",
    "\n",
    "\n",
    "# Step 4: Define the agent node\n",
    "def agent_node(state: AgentState):\n",
    "    \"\"\"The agent node â€” calls the LLM with the current conversation and available tools.\"\"\"\n",
    "    messages = [SystemMessage(content=SYSTEM_PROMPT)] + state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 5: Define the tool node\n",
    "tool_node = ToolNode(all_tools, handle_tool_errors=True)\n",
    "\n",
    "\n",
    "# Step 6: Define routing logic\n",
    "def should_continue(state: AgentState) -> Literal[\"tools\", \"end\"]:\n",
    "    \"\"\"Determine whether to call tools or end the conversation.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"end\"\n",
    "\n",
    "\n",
    "# Step 7: Build the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", \"end\": END})\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile with MemorySaver for short-term memory across turns\n",
    "checkpointer = MemorySaver()\n",
    "agent = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "print(\"Agent compiled with memory and tools!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the graph to confirm our architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for running the agent\n",
    "\n",
    "We'll use a single `thread_id` throughout the notebook so the agent remembers previous interactions (short-term memory via the checkpointer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a consistent thread_id so the agent remembers context across all tasks\n",
    "config = {\"configurable\": {\"thread_id\": \"mcp-workflow-1\"}}\n",
    "\n",
    "\n",
    "async def ask_agent(user_message: str) -> str:\n",
    "    \"\"\"Send a message to the agent and return its final response.\"\"\"\n",
    "    response = await agent.ainvoke(\n",
    "        {\"messages\": [HumanMessage(content=user_message)]},\n",
    "        config,\n",
    "    )\n",
    "    return response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Test the Agent â€” Search & Summarize X Posts\n",
    "\n",
    "Let's put the agent through its paces. First, we'll ask it to search for posts and generate a summary. Because we're using `MemorySaver` with a consistent `thread_id`, the agent will remember the posts it found when we ask it to summarize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am currently unable to retrieve recent posts from @llm_wizard on X/Twitter due to an access limitation. Is there anything else you would like me to assist with?\n"
     ]
    }
   ],
   "source": [
    "# Ask the agent to fetch posts â€” it will use the get_user_posts tool\n",
    "result = await ask_agent(\"Get recent posts from @llm_wizard on X/Twitter.\")\n",
    "print(result[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I currently do not have access to retrieve recent posts from @llm_wizard on X/Twitter, so I am unable to generate a summary report based on their posts.\n",
      "\n",
      "If you have any other requests or if you can provide the posts, I can help create the summary for you.\n"
     ]
    }
   ],
   "source": [
    "# Ask the agent to summarize â€” it remembers the posts from the previous turn!\n",
    "summary = await ask_agent(\n",
    "    \"Now summarize those posts into a structured markdown report with sections for: \"\n",
    "    \"Overview, Key Themes, Notable Posts, and Summary Statistics. \"\n",
    "    \"Format it so it can be saved directly as a summary.md file.\"\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the summary locally for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved to summary.md\n"
     ]
    }
   ],
   "source": [
    "with open(\"summary.md\", \"w\") as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"Summary saved to summary.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â“ Question #1:\n",
    "\n",
    "\n",
    "##### Answer:\n",
    "\n",
    "YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ—ï¸ Activity #1:\n",
    "\n",
    "Your task is to extend the agent with a **new custom X API tool** and verify it works end-to-end.\n",
    "\n",
    "1. **Create a new `@tool` function** called `get_user_profile` that retrieves a user's public profile information using the X API v2 [`GET /2/users/by/username/:username`](https://developer.x.com/en/docs/x-api/users/lookup/api-reference/get-users-by-username-username) endpoint. It should return:\n",
    "   - Display name\n",
    "   - Bio / description\n",
    "   - Follower count\n",
    "   - Following count\n",
    "   - Post count\n",
    "   - Account creation date\n",
    "\n",
    "2. **Rebuild the agent** with the updated tool set â€” add your new tool to `x_api_tools`, re-combine with the MCP tools, re-bind tools to the LLM, and recompile the graph\n",
    "\n",
    "3. **Test it** by asking the agent to:\n",
    "   - Retrieve the profile of an AI thought leader of your choice\n",
    "   - Compare that profile with the posts you already retrieved in Task 5 â€” does the bio match the posting themes?\n",
    "\n",
    "> HINT: The X API v2 user lookup endpoint uses the same Bearer Token authentication. You'll need `user.fields=description,public_metrics,created_at` in your request params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "## actually make it work...\n",
    "@tool\n",
    "def get_user_profile(username: str) -> str:\n",
    "    \"\"\"Get the public profile information for a specific X/Twitter user.\"\"\"\n",
    "    url = f\"https://api.x.com/2/users/by/username/{username}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {os.environ['X_BEARER_TOKEN']}\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "#now add to tools array\n",
    "#rebuild agent graph\n",
    "#test it with query about POTUS\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2: MCP Workflow Through the Agent\n",
    "\n",
    "Now we'll use the same agent to perform all GitHub repository operations through the **GitHub MCP tools**. Because the agent has memory, it already knows the summary it generated in Phase 1.\n",
    "\n",
    "Each task below sends a natural language instruction to the agent. The agent decides which GitHub MCP tool(s) to call to fulfill the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Create a New Repository\n",
    "\n",
    "The agent will use the `create_repository` MCP tool to create a new repo on your GitHub account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await ask_agent(\n",
    "    \"Using your GitHub tools, create a new public repository on my account called \"\n",
    "    \"`x-post-summarizer-2026`. Add a description: 'AI-generated summary of a public \"\n",
    "    \"figure's 2026 X posts, built with LangGraph, MCP tools, and the X API.' \"\n",
    "    \"Initialize it with a README.\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Commit the Summary to Your Repo\n",
    "\n",
    "The agent remembers the summary it generated earlier (short-term memory) and can commit it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await ask_agent(\n",
    "    \"Using your GitHub tools, create a new file called `summary.md` in the \"\n",
    "    \"`x-post-summarizer-2026` repo on the `main` branch. The file should contain \"\n",
    "    \"the X post summary you generated earlier. Use the commit message: \"\n",
    "    \"'Add 2026 X post summary'.\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Create a Feature Branch and Add Metadata\n",
    "\n",
    "The agent will use `create_branch` and `create_or_update_file` MCP tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await ask_agent(\n",
    "    \"Create a new branch called `add-metadata` in my `x-post-summarizer-2026` repo. \"\n",
    "    \"On that branch, create a file called `metadata.json` that contains: the account \"\n",
    "    \"handle analyzed, the date range of posts, the number of posts analyzed, and the \"\n",
    "    \"top 5 themes identified from the summary. Commit it with the message \"\n",
    "    \"'Add analysis metadata'.\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Open a Pull Request\n",
    "\n",
    "The agent will use the `create_pull_request` MCP tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await ask_agent(\n",
    "    \"Open a pull request in my `x-post-summarizer-2026` repo from the `add-metadata` \"\n",
    "    \"branch to `main`. Title it 'Add analysis metadata' and include a description \"\n",
    "    \"summarizing what the metadata file contains.\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Commit the X API Script\n",
    "\n",
    "We'll ask the agent to commit a clean version of the X search script â€” reading credentials from environment variables, no hardcoded keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_search_script = '''import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "BEARER_TOKEN = os.environ.get(\"X_BEARER_TOKEN\")\n",
    "\n",
    "def search_recent_posts(query: str, max_results: int = 20) -> dict:\n",
    "    \"\"\"Search recent X posts using the v2 API.\"\"\"\n",
    "    url = \"https://api.x.com/2/tweets/search/recent\"\n",
    "    headers = {\"Authorization\": f\"Bearer {BEARER_TOKEN}\"}\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"max_results\": min(max_results, 100),\n",
    "        \"tweet.fields\": \"created_at,public_metrics,author_id,text\",\n",
    "        \"expansions\": \"author_id\",\n",
    "        \"user.fields\": \"name,username\",\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def get_user_posts(username: str, max_results: int = 20) -> dict:\n",
    "    \"\"\"Get recent posts from a specific user.\"\"\"\n",
    "    query = f\"from:{username} -is:retweet\"\n",
    "    return search_recent_posts(query, max_results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    handle = sys.argv[1] if len(sys.argv) > 1 else \"llM_wizard\"\n",
    "    print(f\"Searching for recent posts from @{handle}...\")\n",
    "    results = get_user_posts(handle)\n",
    "    with open(\"posts.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    tweets = results.get(\"data\", [])\n",
    "    print(f\"Found {len(tweets)} posts.\")\n",
    "    for tweet in tweets:\n",
    "        print(f\"  [{tweet[\\\"created_at\\\"][:10]}] {tweet[\\\"text\\\"][:100]}...\")\n",
    "'''\n",
    "\n",
    "result = await ask_agent(\n",
    "    f\"Using your GitHub tools, create a new file called `x_search.py` in the \"\n",
    "    f\"`x-post-summarizer-2026` repo on the `main` branch. Use the commit message: \"\n",
    "    f\"'Add X API search script'. Here is the file content:\\n\\n{x_search_script}\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11: Update the README\n",
    "\n",
    "The agent already knows everything about the project from its conversation memory â€” what account was analyzed, how the project was built, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await ask_agent(\n",
    "    \"Update the README.md in my `x-post-summarizer-2026` repo on main to include: \"\n",
    "    \"a project description explaining this repo summarizes a public figure's 2026 X \"\n",
    "    \"posts using AI, the handle analyzed, how the project was built (using a LangGraph \"\n",
    "    \"agent with GitHub MCP tools for repo operations and the X API v2 for post \"\n",
    "    \"retrieval), and instructions for someone else to replicate the process â€” including \"\n",
    "    \"how to set up their X API Bearer Token and install Python dependencies.\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â“ Question #2:\n",
    "\n",
    "Compare using GitHub MCP tools (through a LangGraph agent) to traditional `git` commands. What felt easier? What felt harder or less transparent?\n",
    "\n",
    "##### Answer:\n",
    "* certainly easier, you dont need to remember the syntax\n",
    "* you lose fidelity/power.. for complex workflows\n",
    "\n",
    "* how good is it at understanding exactly what you want\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â“ Question #3:\n",
    "\n",
    "You used MCP for GitHub but wrapped the X API as a `@tool` directly. What are the tradeoffs of consuming an API through an MCP server versus wrapping it as a LangChain tool? When would each approach make more sense?\n",
    "\n",
    "##### Answer:\n",
    "\n",
    "* we had to look up/ know the X api semantics to write the tool, so we had complete control, could validate, error check etc..\n",
    "* Via langchain, it was easier, but we lost control of exact\n",
    "* via langchain, there are security issues perhaps (github likely hs a security proxy behind)\n",
    "* Via langchain, the api was completely abstracted, there are 40+ tools magically exposed\n",
    "\n",
    "Google Agent to Agent vs MCP\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ—ï¸ Activity #1:\n",
    "\n",
    "Your task is to extend the MCP workflow by building a **Multi-Account Comparison Pipeline** through the agent.\n",
    "\n",
    "You are expected to:\n",
    "\n",
    "1. **Retrieve posts from a second X account** â€” choose another public figure or thought leader in a related field\n",
    "\n",
    "2. **Generate a structured comparison** by asking the agent to create a `comparison.md` file that includes:\n",
    "   - Side-by-side topic analysis for both accounts\n",
    "   - Tone and sentiment differences\n",
    "   - Posting frequency comparison\n",
    "   - Top 3 most notable posts from each account\n",
    "   - A brief conclusion about each account's focus area\n",
    "\n",
    "3. **Commit through the MCP workflow**:\n",
    "   - Create a new branch called `add-comparison` in your `x-post-summarizer-2026` repo\n",
    "   - Commit `comparison.md` to that branch\n",
    "   - Open a pull request to merge it into `main`\n",
    "\n",
    "> NOTE: The agent already has memory of the first account's posts from Phase 1. You only need to fetch posts from the second account â€” the agent will use its memory for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
